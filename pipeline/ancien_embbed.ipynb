{
"cells":[
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5d25d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josep\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO validate_candidates: 3 candidats, ref='Des manifestants arborant des croix gammées ont été photogra…'\n",
      "INFO Passe 1 — calcul des embeddings (batch de 4 textes)…\n",
      "INFO Chargement du modèle d'embedding 'paraphrase-multilingual-MiniLM-L12-v2'…\n",
      "INFO Use pytorch device_name: cpu\n",
      "INFO Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SMOKE TEST — Passe 1 (embedding) uniquement\n",
      "(La passe LLM sera ignorée si OPENAI_API_KEY est absente)\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO HTTP Request: HEAD https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/resolve/main/modules.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/e8f8c211226b894fcb81acc59f3b34ba3efd5f42/modules.json \"HTTP/1.1 200 OK\"\n",
      "INFO HTTP Request: HEAD https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/resolve/main/config_sentence_transformers.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/e8f8c211226b894fcb81acc59f3b34ba3efd5f42/config_sentence_transformers.json \"HTTP/1.1 200 OK\"\n",
      "INFO HTTP Request: HEAD https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/resolve/main/config_sentence_transformers.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/e8f8c211226b894fcb81acc59f3b34ba3efd5f42/config_sentence_transformers.json \"HTTP/1.1 200 OK\"\n",
      "INFO HTTP Request: HEAD https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/resolve/main/README.md \"HTTP/1.1 307 Temporary Redirect\"\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "WARNING Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "INFO HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/e8f8c211226b894fcb81acc59f3b34ba3efd5f42/README.md \"HTTP/1.1 200 OK\"\n",
      "INFO HTTP Request: HEAD https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/resolve/main/modules.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/e8f8c211226b894fcb81acc59f3b34ba3efd5f42/modules.json \"HTTP/1.1 200 OK\"\n",
      "INFO HTTP Request: HEAD https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/resolve/main/sentence_bert_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/e8f8c211226b894fcb81acc59f3b34ba3efd5f42/sentence_bert_config.json \"HTTP/1.1 200 OK\"\n",
      "INFO HTTP Request: HEAD https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/resolve/main/adapter_config.json \"HTTP/1.1 404 Not Found\"\n",
      "INFO HTTP Request: HEAD https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/e8f8c211226b894fcb81acc59f3b34ba3efd5f42/config.json \"HTTP/1.1 200 OK\"\n",
      "INFO HTTP Request: HEAD https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/resolve/main/model.safetensors \"HTTP/1.1 302 Found\"\n",
      "INFO HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/xet-read-token/e8f8c211226b894fcb81acc59f3b34ba3efd5f42 \"HTTP/1.1 200 OK\"\n",
      "C:\\Users\\josep\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\josep\\.cache\\huggingface\\hub\\models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 447.08it/s, Materializing param=pooler.dense.weight]                               \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "INFO HTTP Request: HEAD https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/e8f8c211226b894fcb81acc59f3b34ba3efd5f42/config.json \"HTTP/1.1 200 OK\"\n",
      "INFO HTTP Request: HEAD https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/resolve/main/tokenizer_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/e8f8c211226b894fcb81acc59f3b34ba3efd5f42/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
      "INFO HTTP Request: GET https://huggingface.co/api/resolve-cache/models/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/e8f8c211226b894fcb81acc59f3b34ba3efd5f42/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
      "INFO HTTP Request: HEAD https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/resolve/main/tokenizer_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/e8f8c211226b894fcb81acc59f3b34ba3efd5f42/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
      "INFO HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 404 Not Found\"\n",
      "INFO HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/tree/main?recursive=true&expand=false \"HTTP/1.1 200 OK\"\n",
      "INFO HTTP Request: HEAD https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/resolve/main/tokenizer.json \"HTTP/1.1 302 Found\"\n",
      "INFO HTTP Request: HEAD https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/resolve/main/tokenizer.model \"HTTP/1.1 404 Not Found\"\n",
      "INFO HTTP Request: HEAD https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/resolve/main/added_tokens.json \"HTTP/1.1 404 Not Found\"\n",
      "INFO HTTP Request: HEAD https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/resolve/main/special_tokens_map.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/e8f8c211226b894fcb81acc59f3b34ba3efd5f42/special_tokens_map.json \"HTTP/1.1 200 OK\"\n",
      "INFO HTTP Request: GET https://huggingface.co/api/resolve-cache/models/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/e8f8c211226b894fcb81acc59f3b34ba3efd5f42/special_tokens_map.json \"HTTP/1.1 200 OK\"\n",
      "INFO HTTP Request: HEAD https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/resolve/main/chat_template.jinja \"HTTP/1.1 404 Not Found\"\n",
      "INFO HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 \"HTTP/1.1 200 OK\"\n",
      "INFO HTTP Request: HEAD https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/resolve/main/1_Pooling/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "INFO HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/e8f8c211226b894fcb81acc59f3b34ba3efd5f42/1_Pooling%2Fconfig.json \"HTTP/1.1 200 OK\"\n",
      "INFO HTTP Request: GET https://huggingface.co/api/resolve-cache/models/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/e8f8c211226b894fcb81acc59f3b34ba3efd5f42/1_Pooling%2Fconfig.json \"HTTP/1.1 200 OK\"\n",
      "INFO HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 \"HTTP/1.1 200 OK\"\n",
      "INFO Modèle d'embedding chargé.\n",
      "INFO   Candidat[0] passe le pré-filtrage  embed=0.533  'Le correspondant de BFM TV à Lyon confirme avoir v…'\n",
      "INFO   Candidat[1] éliminé par embedding  embed=-0.074  'Recette de quiche lorraine maison : 200g de lardon…'\n",
      "INFO   Candidat[2] passe le pré-filtrage  embed=0.509  'Les organisateurs du rassemblement lyonnais dément…'\n",
      "INFO Passe 1 terminée : 2/3 candidats survivants (seuil=0.38).\n",
      "INFO Passe 2 — validation LLM sur 2 candidats (workers=4)…\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note : OPENAI_API_KEY non définie. La passe LLM (étape 4) nécessite une clé OpenAI valide.\n",
      "Test de la passe embedding seule…\n",
      "\n",
      "  [0.5331] ✓ survit  @similar_post\n",
      "  [-0.0736] ✗ éliminé  @unrelated_post\n",
      "  [0.5086] ✓ survit  @moderate_post\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "step4_validate_candidates.py — Étape 4 de la pipeline\n",
    "=======================================================\n",
    "Validation sémantique des posts candidats par double passe :\n",
    "\n",
    "  PASSE 1 — Embedding cosinus (sentence-transformers, batch)\n",
    "            Modèle : paraphrase-multilingual-MiniLM-L12-v2\n",
    "            Objectif : pré-filtrer à bas coût, éliminer les faux candidats évidents.\n",
    "            Seuil   : EMBED_PREFILTER_THRESHOLD (défaut 0.38)\n",
    "\n",
    "  PASSE 2 — LLM (OpenAI gpt-4o-mini, JSON structuré)\n",
    "            Objectif : évaluer la similarité de CLAIM, pas seulement de surface.\n",
    "            Chaque appel est indépendant → parallélisable via ThreadPoolExecutor.\n",
    "\n",
    "  SCORE FINAL = EMBED_WEIGHT * embed_score + LLM_WEIGHT * llm_score\n",
    "  Retour      : [(BlueskyPost, final_score)] filtrés à ≥ FINAL_THRESHOLD,\n",
    "                triés par score décroissant.\n",
    "\n",
    "Dépendances :\n",
    "    pip install sentence-transformers openai\n",
    "\n",
    "Variables d'environnement :\n",
    "    OPENAI_API_KEY   — clé OpenAI (obligatoire pour la passe LLM)\n",
    "    OPENAI_MODEL     — modèle OpenAI (défaut: gpt-4o-mini)\n",
    "    VALIDATE_WORKERS — nombre de threads parallèles pour les appels LLM (défaut: 4)\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from openai import OpenAI\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Configuration\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# Modèle d'embedding : multilingue, gère le français, ~117 MB\n",
    "EMBEDDING_MODEL_NAME = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "\n",
    "# Seuil de pré-filtrage par embedding. En dessous → pas d'appel LLM.\n",
    "# 0.38 est volontairement bas pour ne pas rater de vrais positifs\n",
    "# (le LLM tranchera ensuite).\n",
    "EMBED_PREFILTER_THRESHOLD = 0.38\n",
    "\n",
    "# Poids du score final. LLM > embedding car il comprend l'intention sémantique.\n",
    "EMBED_WEIGHT = 0.35\n",
    "LLM_WEIGHT   = 0.65\n",
    "\n",
    "# Seuil de validation finale. Conforme à MIN_SIMILARITY de provenance_graph.py.\n",
    "FINAL_THRESHOLD = 0.65\n",
    "\n",
    "# Parallélisme des appels LLM (ThreadPoolExecutor)\n",
    "DEFAULT_WORKERS = int(os.getenv(\"VALIDATE_WORKERS\", \"4\"))\n",
    "\n",
    "# Modèle OpenAI\n",
    "OPENAI_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Logging\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Résultat intermédiaire de validation\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "@dataclass\n",
    "class _ValidationResult:\n",
    "    \"\"\"Résultat intermédiaire — usage interne uniquement.\"\"\"\n",
    "    post: object                     # BlueskyPost\n",
    "    embed_score: float               # cosinus brut\n",
    "    llm_score: Optional[float]       # score LLM (None si non appelé ou erreur)\n",
    "    llm_reasoning: str = \"\"          # justification textuelle du LLM\n",
    "    final_score: float = 0.0         # score combiné calculé après les deux passes\n",
    "\n",
    "    def compute_final(self) -> None:\n",
    "        \"\"\"\n",
    "        Calcule le score final.\n",
    "        Si le LLM n'a pas répondu (None), on utilise uniquement l'embedding\n",
    "        mais on plafonne à FINAL_THRESHOLD - ε pour ne jamais valider\n",
    "        sur embedding seul (comportement conservateur).\n",
    "        \"\"\"\n",
    "        if self.llm_score is None:\n",
    "            # Passe LLM échouée → on utilise l'embedding avec un facteur pénalisant\n",
    "            self.final_score = self.embed_score * 0.80\n",
    "        else:\n",
    "            self.final_score = (\n",
    "                EMBED_WEIGHT * self.embed_score\n",
    "                + LLM_WEIGHT * self.llm_score\n",
    "            )\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Singleton du modèle d'embedding (chargé une seule fois par processus)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "_embedding_model: Optional[SentenceTransformer] = None\n",
    "\n",
    "def _get_embedding_model() -> SentenceTransformer:\n",
    "    \"\"\"Charge le modèle d'embedding de façon paresseuse (lazy singleton).\"\"\"\n",
    "    global _embedding_model\n",
    "    if _embedding_model is None:\n",
    "        logger.info(\"Chargement du modèle d'embedding '%s'…\", EMBEDDING_MODEL_NAME)\n",
    "        _embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "        logger.info(\"Modèle d'embedding chargé.\")\n",
    "    return _embedding_model\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Passe 1 — Embeddings\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def _cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    \"\"\"Similarité cosinus entre deux vecteurs numpy 1D.\"\"\"\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    if norm_a == 0.0 or norm_b == 0.0:\n",
    "        return 0.0\n",
    "    return float(np.dot(a, b) / (norm_a * norm_b))\n",
    "\n",
    "\n",
    "def _embed_batch(texts: list[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Encode une liste de textes en une seule inférence batch.\n",
    "    Retourne un tableau de shape (N, embedding_dim).\n",
    "    \"\"\"\n",
    "    model = _get_embedding_model()\n",
    "    # normalize_embeddings=True → les vecteurs sont L2-normalisés,\n",
    "    # le produit scalaire devient directement la similarité cosinus.\n",
    "    return model.encode(texts, normalize_embeddings=True, show_progress_bar=False)\n",
    "\n",
    "\n",
    "def _pass1_embedding(\n",
    "    ref_text: str,\n",
    "    candidate_texts: list[str],\n",
    ") -> list[float]:\n",
    "    \"\"\"\n",
    "    Calcule les similarités cosinus entre ref_text et chaque candidat.\n",
    "    Retourne une liste de scores dans [0, 1] (même ordre que candidate_texts).\n",
    "    \"\"\"\n",
    "    all_texts = [ref_text] + candidate_texts\n",
    "    embeddings = _embed_batch(all_texts)\n",
    "\n",
    "    ref_emb   = embeddings[0]          # shape (dim,)\n",
    "    cand_embs = embeddings[1:]         # shape (N, dim)\n",
    "\n",
    "    # Produit scalaire vectorisé (tous les candidats d'un coup)\n",
    "    similarities = (cand_embs @ ref_emb).tolist()  # liste de N floats\n",
    "    return similarities\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Passe 2 — LLM\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "_LLM_SYSTEM_PROMPT = \"\"\"Tu es un expert en analyse de désinformation et de propagation de l'information sur les réseaux sociaux.\n",
    "\n",
    "Ta tâche est de comparer deux posts Bluesky et de déterminer s'ils parlent du MÊME FAIT ou de la MÊME AFFIRMATION principale.\n",
    "\n",
    "Règles d'évaluation :\n",
    "- Concentre-toi sur l'AFFIRMATION FACTUELLE centrale, pas sur la forme, le style ou le contexte éditorial.\n",
    "- Deux posts peuvent utiliser des mots très différents mais rapporter le même fait → score élevé.\n",
    "- Une reformulation partielle ou biaisée du même fait doit recevoir un score élevé (c'est précisément ce qu'on cherche à détecter).\n",
    "- Des posts sur des sujets superficiellement similaires mais avec des faits différents → score bas.\n",
    "\n",
    "Tu dois répondre UNIQUEMENT en JSON valide, sans aucun texte autour, avec exactement ces clés :\n",
    "{\n",
    "  \"same_claim\": <true|false>,\n",
    "  \"similarity_score\": <float entre 0.0 et 1.0>,\n",
    "  \"reasoning\": \"<explication concise en français, max 80 mots>\"\n",
    "}\"\"\"\n",
    "\n",
    "_LLM_USER_TEMPLATE = \"\"\"POST DE RÉFÉRENCE (plus récent) :\n",
    "\\\"\\\"\\\"\n",
    "{ref_text}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "POST CANDIDAT (antérieur) :\n",
    "\\\"\\\"\\\"\n",
    "{candidate_text}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "Ces deux posts rapportent-ils le même fait ou la même affirmation principale ?\"\"\"\n",
    "\n",
    "\n",
    "def _parse_llm_response(raw: str) -> tuple[float, str]:\n",
    "    \"\"\"\n",
    "    Parse la réponse JSON du LLM.\n",
    "    Retourne (similarity_score, reasoning).\n",
    "    En cas d'erreur de parsing, retourne (-1.0, \"parse_error\").\n",
    "    \"\"\"\n",
    "    # Extraction robuste : on cherche le premier bloc JSON même si le LLM\n",
    "    # a ajouté du texte malgré les instructions.\n",
    "    json_match = re.search(r'\\{.*\\}', raw, re.DOTALL)\n",
    "    if not json_match:\n",
    "        logger.warning(\"LLM: aucun JSON trouvé dans la réponse : %s\", raw[:200])\n",
    "        return -1.0, \"parse_error\"\n",
    "\n",
    "    try:\n",
    "        data = json.loads(json_match.group())\n",
    "    except json.JSONDecodeError as exc:\n",
    "        logger.warning(\"LLM: JSON invalide (%s) : %s\", exc, raw[:200])\n",
    "        return -1.0, \"json_decode_error\"\n",
    "\n",
    "    # Validation et coercition du score\n",
    "    score = data.get(\"similarity_score\", -1.0)\n",
    "    try:\n",
    "        score = float(score)\n",
    "        score = max(0.0, min(1.0, score))   # clamp [0, 1]\n",
    "    except (TypeError, ValueError):\n",
    "        logger.warning(\"LLM: similarity_score invalide : %s\", score)\n",
    "        return -1.0, \"invalid_score\"\n",
    "\n",
    "    reasoning = str(data.get(\"reasoning\", \"\"))\n",
    "    return score, reasoning\n",
    "\n",
    "\n",
    "def _call_llm_single(\n",
    "    client: OpenAI,\n",
    "    ref_text: str,\n",
    "    candidate_text: str,\n",
    "    candidate_idx: int,\n",
    ") -> tuple[int, float, str]:\n",
    "    \"\"\"\n",
    "    Appel LLM pour une seule paire (ref, candidat).\n",
    "    Retourne (candidate_idx, llm_score, reasoning).\n",
    "    llm_score == -1.0 en cas d'échec.\n",
    "    \"\"\"\n",
    "    prompt = _LLM_USER_TEMPLATE.format(\n",
    "        ref_text=ref_text.strip(),\n",
    "        candidate_text=candidate_text.strip(),\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=OPENAI_MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": _LLM_SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\",   \"content\": prompt},\n",
    "            ],\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "            temperature=0.0,    # déterministe\n",
    "            max_tokens=256,\n",
    "            timeout=20.0,\n",
    "        )\n",
    "        raw = response.choices[0].message.content or \"\"\n",
    "        score, reasoning = _parse_llm_response(raw)\n",
    "        logger.debug(\"LLM[%d]: score=%.2f  reasoning=%s\", candidate_idx, score, reasoning[:60])\n",
    "        return candidate_idx, score, reasoning\n",
    "\n",
    "    except Exception as exc:\n",
    "        logger.warning(\"LLM[%d]: appel échoué (%s)\", candidate_idx, exc)\n",
    "        return candidate_idx, -1.0, f\"api_error: {exc}\"\n",
    "\n",
    "\n",
    "def _pass2_llm_parallel(\n",
    "    ref_text: str,\n",
    "    candidates: list[tuple[int, str]],   # [(original_idx, text), ...]\n",
    "    workers: int = DEFAULT_WORKERS,\n",
    ") -> dict[int, tuple[float, str]]:\n",
    "    \"\"\"\n",
    "    Lance les appels LLM en parallèle pour tous les candidats pré-filtrés.\n",
    "    Retourne un dict {original_idx: (llm_score, reasoning)}.\n",
    "    \"\"\"\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise EnvironmentError(\n",
    "            \"OPENAI_API_KEY non définie. \"\n",
    "            \"La passe LLM (étape 4) nécessite une clé OpenAI valide.\"\n",
    "        )\n",
    "\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    results: dict[int, tuple[float, str]] = {}\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=workers) as executor:\n",
    "        futures = {\n",
    "            executor.submit(\n",
    "                _call_llm_single, client, ref_text, text, orig_idx\n",
    "            ): orig_idx\n",
    "            for orig_idx, text in candidates\n",
    "        }\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            orig_idx = futures[future]\n",
    "            try:\n",
    "                _, llm_score, reasoning = future.result()\n",
    "            except Exception as exc:\n",
    "                logger.error(\"Future[%d] inattendue : %s\", orig_idx, exc)\n",
    "                llm_score, reasoning = -1.0, \"unexpected_error\"\n",
    "\n",
    "            results[orig_idx] = (llm_score, reasoning)\n",
    "            logger.info(\n",
    "                \"  Candidat[%d] → LLM score=%.2f  (%s)\",\n",
    "                orig_idx, llm_score, reasoning[:50]\n",
    "            )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Fonction principale — API publique\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def validate_candidates(\n",
    "    ref_post,\n",
    "    candidates: list,\n",
    "    embed_prefilter_threshold: float = EMBED_PREFILTER_THRESHOLD,\n",
    "    final_threshold: float = FINAL_THRESHOLD,\n",
    "    workers: int = DEFAULT_WORKERS,\n",
    ") -> list[tuple[object, float]]:\n",
    "    \"\"\"\n",
    "    Étape 4 — Validation sémantique des posts candidats.\n",
    "\n",
    "    Args:\n",
    "        ref_post  : BlueskyPost de référence (le post dont on cherche les antécédents).\n",
    "                    Doit exposer un attribut `.text` (str).\n",
    "        candidates: Liste de BlueskyPost candidats (issus de l'étape 3).\n",
    "                    Chaque objet doit exposer `.text`.\n",
    "        embed_prefilter_threshold:\n",
    "                    Seuil de pré-filtrage par similarité cosinus.\n",
    "                    Les candidats en dessous ne sont pas soumis au LLM.\n",
    "                    Défaut : 0.38 (conservateur).\n",
    "        final_threshold:\n",
    "                    Seuil de validation finale (score combiné).\n",
    "                    Défaut : 0.65, conforme à MIN_SIMILARITY du graphe.\n",
    "        workers   : Nombre de threads parallèles pour les appels LLM.\n",
    "\n",
    "    Returns:\n",
    "        Liste de (BlueskyPost, final_score) pour les candidats validés,\n",
    "        triée par final_score décroissant.\n",
    "        Seuls les candidats avec final_score >= final_threshold sont inclus.\n",
    "\n",
    "    Raises:\n",
    "        EnvironmentError : si OPENAI_API_KEY n'est pas définie et que\n",
    "                           des candidats ont survécu au pré-filtrage.\n",
    "    \"\"\"\n",
    "    if not candidates:\n",
    "        logger.info(\"validate_candidates: aucun candidat → retour vide immédiat.\")\n",
    "        return []\n",
    "\n",
    "    ref_text = ref_post.text\n",
    "    candidate_texts = [c.text for c in candidates]\n",
    "\n",
    "    logger.info(\n",
    "        \"validate_candidates: %d candidats, ref='%s…'\",\n",
    "        len(candidates), ref_text[:60]\n",
    "    )\n",
    "\n",
    "    # ── Passe 1 : Embedding ────────────────────────────────────────────────\n",
    "    logger.info(\"Passe 1 — calcul des embeddings (batch de %d textes)…\", len(candidates) + 1)\n",
    "    embed_scores = _pass1_embedding(ref_text, candidate_texts)\n",
    "\n",
    "    # Pré-filtrage\n",
    "    prefiltered: list[tuple[int, str]] = []   # (original_index, text)\n",
    "    validation_results: list[_ValidationResult] = []\n",
    "\n",
    "    for idx, (post, embed_score) in enumerate(zip(candidates, embed_scores)):\n",
    "        result = _ValidationResult(post=post, embed_score=embed_score, llm_score=None)\n",
    "        validation_results.append(result)\n",
    "\n",
    "        if embed_score >= embed_prefilter_threshold:\n",
    "            prefiltered.append((idx, candidate_texts[idx]))\n",
    "            logger.info(\n",
    "                \"  Candidat[%d] passe le pré-filtrage  embed=%.3f  '%s…'\",\n",
    "                idx, embed_score, candidate_texts[idx][:50]\n",
    "            )\n",
    "        else:\n",
    "            logger.info(\n",
    "                \"  Candidat[%d] éliminé par embedding  embed=%.3f  '%s…'\",\n",
    "                idx, embed_score, candidate_texts[idx][:50]\n",
    "            )\n",
    "\n",
    "    logger.info(\n",
    "        \"Passe 1 terminée : %d/%d candidats survivants (seuil=%.2f).\",\n",
    "        len(prefiltered), len(candidates), embed_prefilter_threshold\n",
    "    )\n",
    "\n",
    "    # ── Passe 2 : LLM (uniquement sur les survivants) ─────────────────────\n",
    "    if prefiltered:\n",
    "        logger.info(\n",
    "            \"Passe 2 — validation LLM sur %d candidats (workers=%d)…\",\n",
    "            len(prefiltered), workers\n",
    "        )\n",
    "        llm_results = _pass2_llm_parallel(ref_text, prefiltered, workers=workers)\n",
    "\n",
    "        for orig_idx, (llm_score, reasoning) in llm_results.items():\n",
    "            r = validation_results[orig_idx]\n",
    "            # llm_score == -1.0 signifie un échec d'appel → on laisse None\n",
    "            r.llm_score   = llm_score if llm_score >= 0.0 else None\n",
    "            r.llm_reasoning = reasoning\n",
    "    else:\n",
    "        logger.info(\"Passe 2 — aucun candidat à soumettre au LLM.\")\n",
    "\n",
    "    # ── Score final + filtrage ─────────────────────────────────────────────\n",
    "    validated: list[tuple[object, float]] = []\n",
    "\n",
    "    for idx, result in enumerate(validation_results):\n",
    "        result.compute_final()\n",
    "        logger.info(\n",
    "            \"  Candidat[%d] : embed=%.3f  llm=%s  final=%.3f  → %s\",\n",
    "            idx,\n",
    "            result.embed_score,\n",
    "            f\"{result.llm_score:.3f}\" if result.llm_score is not None else \"N/A\",\n",
    "            result.final_score,\n",
    "            \"✓ VALIDÉ\" if result.final_score >= final_threshold else \"✗ rejeté\",\n",
    "        )\n",
    "\n",
    "        if result.final_score >= final_threshold:\n",
    "            validated.append((result.post, round(result.final_score, 4)))\n",
    "\n",
    "    # Tri par score décroissant\n",
    "    validated.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    logger.info(\n",
    "        \"validate_candidates : %d/%d candidats validés (seuil final=%.2f).\",\n",
    "        len(validated), len(candidates), final_threshold\n",
    "    )\n",
    "    return validated\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Tests unitaires légers (sans dépendance externe)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Smoke test local — valide la passe embedding sans appel LLM.\n",
    "    Lance : python step4_validate_candidates.py\n",
    "    \"\"\"\n",
    "    import logging\n",
    "    logging.basicConfig(level=logging.INFO, format=\"%(levelname)s %(message)s\")\n",
    "\n",
    "    # ── Stub minimal de BlueskyPost ────────────────────────────────────────\n",
    "    class _FakePost:\n",
    "        def __init__(self, text: str, handle: str = \"user\"):\n",
    "            self.text          = text\n",
    "            self.author_handle = handle\n",
    "            self.uri           = f\"at://fake/{handle}\"\n",
    "            self.date          = \"2025-01-01T00:00:00Z\"\n",
    "            self.likes         = 0\n",
    "            self.reposts       = 0\n",
    "            self.replies       = 0\n",
    "\n",
    "    ref = _FakePost(\n",
    "        \"Des manifestants arborant des croix gammées ont été photographiés lors \"\n",
    "        \"du rassemblement de Lyon selon le journaliste de BFM présent sur place.\",\n",
    "        handle=\"ref_post\"\n",
    "    )\n",
    "\n",
    "    candidates = [\n",
    "        _FakePost(\n",
    "            # Très similaire — doit passer le pré-filtrage embedding\n",
    "            \"Le correspondant de BFM TV à Lyon confirme avoir vu des symboles nazis \"\n",
    "            \"parmi les participants au rassemblement.\",\n",
    "            handle=\"similar_post\"\n",
    "        ),\n",
    "        _FakePost(\n",
    "            # Sujet différent — doit être éliminé par embedding\n",
    "            \"Recette de quiche lorraine maison : 200g de lardons, 3 œufs, 20cl de crème.\",\n",
    "            handle=\"unrelated_post\"\n",
    "        ),\n",
    "        _FakePost(\n",
    "            # Modérément similaire — résultat dépend du LLM\n",
    "            \"Les organisateurs du rassemblement lyonnais démentent toute présence \"\n",
    "            \"d'éléments d'extrême droite lors de l'événement.\",\n",
    "            handle=\"moderate_post\"\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SMOKE TEST — Passe 1 (embedding) uniquement\")\n",
    "    print(\"(La passe LLM sera ignorée si OPENAI_API_KEY est absente)\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    
    "    try:\n",
    "        results = validate_candidates(ref, candidates)\n",
    "        print(f\"\\n→ {len(results)} candidat(s) validé(s) :\\n\")\n",
    "        for post, score in results:\n",
    "            print(f\"  [{score:.4f}]  @{post.author_handle} : {post.text[:80]}…\")\n",
    "    except EnvironmentError as e:\n",
    "        # OPENAI_API_KEY manquante — on teste uniquement l'embedding\n",
    "        print(f\"Note : {e}\")\n",
    "        print(\"Test de la passe embedding seule…\\n\")\n",
    "        scores = _pass1_embedding(ref.text, [c.text for c in candidates])\n",
    "        for c, s in zip(candidates, scores):\n",
    "            status = \"✓ survit\" if s >= EMBED_PREFILTER_THRESHOLD else \"✗ éliminé\"\n",
    "            print(f\"  [{s:.4f}] {status}  @{c.author_handle}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5

}