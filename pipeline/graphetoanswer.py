"""
synthesizer.py â€” Ã‰tape 9 de la pipeline TrustGraph
====================================================
SynthÃ¨se narrative du parcours de l'information Ã  partir d'un
ProvenanceGraph (Ã©tape 6) dÃ©jÃ  construit.

Extrait TOUS les chemins (source primaire â†’ post racine) depuis le DiGraph
NetworkX, puis appelle Gemini pour produire une analyse structurÃ©e couvrant
l'ensemble du mouvement d'information â€” pas seulement une branche.

Place dans la pipeline :
    provenance_graph.py  â†’  synthesizer.py  â†’  [visualisation / export]

Usage minimal :
    from provenance_graph import ProvenanceGraph
    from synthesizer import synthesize

    graph = ProvenanceGraph()
    graph.build("https://bsky.app/profile/alice.bsky.social/post/3abc")
    result = synthesize(graph)
    print(result.narrative)
    result.save_json("synthesis.json")

Retourne un SynthesisResult contenant :
    original_fact    â€” le fait tel qu'exprimÃ© par la source primaire
    final_version    â€” le fait tel qu'il apparaÃ®t dans le post racine
    distortions      â€” dÃ©formations step-by-step entre chaque paire de nÅ“uds
    verdict          â€” Ã©valuation globale de la vÃ©racitÃ© du post racine
    veracity_score   â€” score global 0-1 estimÃ© par le LLM
    narrative        â€” paragraphe de synthÃ¨se lisible par un humain
    top_amplifiers   â€” handles ayant le plus contribuÃ© Ã  la dÃ©sinformation

DÃ©pendances :
    pip install google-genai networkx

Variables d'environnement :
    GOOGLE_API_KEY    â€” clÃ© API Gemini (obligatoire)
    SYNTHESIS_MODEL   â€” modÃ¨le Gemini Ã  utiliser (dÃ©faut : gemini-2.5-flash)
"""

from __future__ import annotations

import json
import logging
import math
import os
import re
import time
from dataclasses import dataclass, field
from typing import Optional

import networkx as nx

from pipeline.Provenance_graph_etape6 import ProvenanceGraph

from google import genai
from google.genai import errors as genai_errors
from google.genai import types as genai_types

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Configuration
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

SYNTHESIS_MODEL = os.getenv("SYNTHESIS_MODEL", "gemini-2.5-flash")

_MAX_RETRIES = 3
_RETRY_BASE_DELAY = 2.0

# SÃ©lection des chemins pour le prompt LLM
# Un chemin = un couple (source_primaire â†’ racine)
# On limite le nombre de chemins envoyÃ©s Ã  Gemini pour Ã©viter le bruit et le
# gonflement du prompt. Les chemins Ã©cartÃ©s sont les moins fiables ET/OU
# redondants par rapport aux chemins dÃ©jÃ  sÃ©lectionnÃ©s.
_MAX_PATHS_IN_PROMPT    = 4     # Nombre max de chemins envoyÃ©s Ã  Gemini
_LOW_FIABILITY_THRESH   = 0.35  # En dessous : source peu fiable
_HIGH_VIRALITY_THRESH   = 0.55  # Au dessus  : source virale (risque de bruit)

logger = logging.getLogger(__name__)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Prompts
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

_SYSTEM_PROMPT = """Tu es un expert en fact-checking et en analyse de dÃ©sinformation sur les rÃ©seaux sociaux.

Ta tÃ¢che est d'analyser l'ensemble des chemins de propagation d'une information sur Bluesky \
(de chaque source primaire vers le post analysÃ©) et de produire une synthÃ¨se rigoureuse et objective \
de la maniÃ¨re dont l'information a Ã©voluÃ©, Ã©tÃ© dÃ©formÃ©e ou amplifiÃ©e au fil de sa propagation.

Si plusieurs sources primaires existent, tu dois les prendre TOUTES en compte pour Ã©valuer \
la vÃ©racitÃ© globale du post analysÃ©, en pondÃ©rant selon leur fiabilitÃ©.

Principes directeurs :
- Reste strictement factuel et neutre. N'invente rien d'absent des donnÃ©es fournies.
- Appuie-toi sur les scores d'engagement fournis pour identifier les vecteurs de propagation.
- Si l'information n'a PAS Ã©tÃ© dÃ©formÃ©e, dis-le explicitement.
- Identifie prÃ©cisÃ©ment QUI a introduit quelle dÃ©formation et COMMENT.
- Le champ "narrative" doit Ãªtre comprÃ©hensible par un journaliste non-technique.
- Un nÅ“ud marquÃ© "SOURCE DE CONFIANCE" doit Ãªtre traitÃ© comme une rÃ©fÃ©rence fiable.
- Un nÅ“ud marquÃ© "SOURCE PRIMAIRE" est l'origine ; ses claims sont la vÃ©ritÃ© de rÃ©fÃ©rence.
- Les scores de distorsion sur les arÃªtes (0 = fidÃ¨le, 1 = trÃ¨s dÃ©formÃ©) sont calculÃ©s \
  objectivement par la pipeline â€” appuie-toi dessus pour Ã©tayer ton analyse.

RÃ©ponds UNIQUEMENT en JSON valide, sans texte autour, avec exactement ces clÃ©s :
{
  "original_fact": "<le fait tel qu'exprimÃ© par la/les source(s) primaire(s), en une phrase>",
  "final_version": "<le fait tel qu'il apparaÃ®t dans le post analysÃ©, en une phrase>",
  "distortions": [
    {
      "step": <entier, 1 = premier relais aprÃ¨s la source>,
      "from_author": "<handle du post amont>",
      "to_author":   "<handle du post aval>",
      "what_changed": "<description prÃ©cise de la dÃ©formation, max 100 mots>",
      "info_lost":    ["<information omise>"],
      "info_added":   ["<information ajoutÃ©e ou inventÃ©e>"],
      "tone_shift":   "<neutral|more_alarmist|more_partisan|minimizes|inverts>",
      "distortion_score": <float 0.0-1.0>
    }
  ],
  "verdict": "<Ã©valuation globale en 1-2 phrases : le post est-il fidÃ¨le Ã  la source ?>",
  "veracity_score": <float 0.0-1.0, vÃ©racitÃ© du post racine par rapport Ã  la/les source(s)>,
  "narrative": "<paragraphe de 100-180 mots rÃ©digÃ© pour un lecteur non-technique>",
  "top_amplifiers": ["<handle du compte ayant le plus propagÃ© une version dÃ©formÃ©e>"]
}

RÃ¨gles strictes :
- "distortions" = [] si l'information n'a pas Ã©tÃ© dÃ©formÃ©e.
- "top_amplifiers" = [] si aucun compte n'a propagÃ© de dÃ©sinformation.
- Tous les floats doivent Ãªtre dans [0.0, 1.0].
- Ne jamais inventer d'informations absentes de la chaÃ®ne fournie."""


_USER_TEMPLATE = """Voici l'ensemble des chemins de propagation d'une information sur Bluesky, \
depuis chaque source primaire identifiÃ©e jusqu'au post analysÃ©.

{all_paths_text}

{graph_structure}

Analyse l'intÃ©gralitÃ© de ces chemins. Si plusieurs sources primaires existent, \
prends-les TOUTES en compte pour Ã©valuer la vÃ©racitÃ© globale du post analysÃ©. \
Produis la synthÃ¨se JSON demandÃ©e en couvrant le mouvement d'information \
dans sa totalitÃ©, pas seulement une branche."""


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ModÃ¨les de donnÃ©es â€” Sortie
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@dataclass
class Distortion:
    """DÃ©formation de l'information entre deux nÅ“uds consÃ©cutifs de la chaÃ®ne."""
    step:             int
    from_author:      str
    to_author:        str
    what_changed:     str
    info_lost:        list[str] = field(default_factory=list)
    info_added:       list[str] = field(default_factory=list)
    tone_shift:       str = "neutral"
    distortion_score: float = 0.0

    def to_dict(self) -> dict:
        return {
            "step":             self.step,
            "from_author":      self.from_author,
            "to_author":        self.to_author,
            "what_changed":     self.what_changed,
            "info_lost":        self.info_lost,
            "info_added":       self.info_added,
            "tone_shift":       self.tone_shift,
            "distortion_score": self.distortion_score,
        }


@dataclass
class SynthesisResult:
    """RÃ©sultat complet de l'Ã©tape 9."""
    original_fact:  str
    final_version:  str
    distortions:    list[Distortion] = field(default_factory=list)
    verdict:        str = ""
    veracity_score: float = 0.5
    narrative:      str = ""
    top_amplifiers: list[str] = field(default_factory=list)

    def to_dict(self) -> dict:
        return {
            "original_fact":  self.original_fact,
            "final_version":  self.final_version,
            "distortions":    [d.to_dict() for d in self.distortions],
            "verdict":        self.verdict,
            "veracity_score": self.veracity_score,
            "narrative":      self.narrative,
            "top_amplifiers": self.top_amplifiers,
        }

    def save_json(self, path: str) -> None:
        with open(path, "w", encoding="utf-8") as f:
            json.dump(self.to_dict(), f, indent=2, ensure_ascii=False)
        print(f"ðŸ’¾ SynthÃ¨se sauvegardÃ©e : {path}")

    def print_summary(self) -> None:
        """Affiche un rÃ©sumÃ© lisible dans le terminal."""
        bar = "â•" * 60
        score = self.veracity_score
        icon = "ðŸŸ¢" if score >= 0.75 else ("ðŸŸ¡" if score >= 0.45 else "ðŸ”´")

        print(f"\n{bar}")
        print("ðŸ“‹  SYNTHÃˆSE â€” PARCOURS DE L'INFORMATION")
        print(bar)
        print(f"\n{icon}  Score de vÃ©racitÃ© global : {score:.2f} / 1.00")
        print(f"\nðŸ“Œ  Fait original  : {self.original_fact}")
        print(f"ðŸ“Œ  Version finale : {self.final_version}")

        if self.distortions:
            print(f"\nâš ï¸   DÃ©formations ({len(self.distortions)}) :")
            for d in self.distortions:
                print(f"\n  [{d.step}] @{d.from_author} â†’ @{d.to_author}"
                      f"  distortion={d.distortion_score:.2f}  tone={d.tone_shift}")
                print(f"      {d.what_changed}")
                if d.info_lost:
                    print(f"      Perdu  : {', '.join(d.info_lost)}")
                if d.info_added:
                    print(f"      AjoutÃ© : {', '.join(d.info_added)}")
        else:
            print("\nâœ…  Aucune dÃ©formation significative dÃ©tectÃ©e.")

        if self.top_amplifiers:
            print(f"\nðŸ”Š  Principaux amplificateurs : "
                  f"{', '.join('@' + h for h in self.top_amplifiers)}")

        print(f"\nðŸ’¬  Verdict : {self.verdict}")
        print(f"\nðŸ“–  Narrative :\n    {self.narrative}")
        print(f"\n{bar}\n")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Client Gemini
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _get_client() -> genai.Client:
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        raise EnvironmentError(
            "GOOGLE_API_KEY non dÃ©finie. "
            "Exporter la variable avant de lancer le script."
        )
    return genai.Client(api_key=api_key)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Extraction de TOUS les chemins depuis le ProvenanceGraph
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _extract_all_paths(graph) -> list[tuple[list[dict], list[dict]]]:
    """
    Extrait TOUS les chemins [source_primaire â†’ post_racine] depuis le DiGraph
    du ProvenanceGraph, un chemin par couple (primaire, racine) distinct.

    Contrairement Ã  l'ancienne approche (remontÃ©e d'une seule branche via
    G.predecessors), cette fonction utilise nx.shortest_path pour trouver
    chaque chemin existant dans le graphe, donnant au LLM une vue complÃ¨te
    du mouvement d'information Ã  travers toutes les branches.

    Returns:
        Liste de (chain_nodes, chain_edges) dans l'ordre [source â†’ racine]
        pour chaque chemin distinct. Toujours non vide.

    Raises:
        ValueError : graphe vide.
    """
    G = graph.G
    if isinstance(G, (nx.MultiDiGraph, nx.MultiGraph)):
        G = nx.DiGraph(G)

    if G.number_of_nodes() == 0:
        raise ValueError("Le graphe est vide â€” impossible d'extraire des chemins.")

    # â”€â”€ Racines (is_root=True ou depth minimal) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    roots = [nid for nid, d in G.nodes(data=True) if d.get("is_root")]
    if not roots:
        roots = [min(G.nodes(data=True), key=lambda x: x[1].get("depth", 0))[0]]
        logger.warning("Aucun nÅ“ud is_root=True â€” fallback sur depth minimal : %s", roots[0])

    # â”€â”€ Sources primaires (is_primary=True ou sans prÃ©dÃ©cesseur) â”€â”€â”€â”€â”€â”€â”€â”€â”€
    primaries = [nid for nid, d in G.nodes(data=True) if d.get("is_primary")]
    if not primaries:
        primaries = [nid for nid in G.nodes() if not list(G.predecessors(nid))]
    if not primaries:
        primaries = roots  # dernier recours

    # â”€â”€ Extraction des chemins â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    all_paths: list[tuple[list[dict], list[dict]]] = []
    seen: set[tuple] = set()

    for primary in primaries:
        for root in roots:
            try:
                path_nids = nx.shortest_path(G, source=primary, target=root)
            except nx.NetworkXNoPath:
                continue

            path_key = tuple(path_nids)
            if path_key in seen:
                continue
            seen.add(path_key)

            chain_nodes: list[dict] = []
            for nid in path_nids:
                data = dict(G.nodes[nid])
                data["uri"] = data.get("uri", nid)
                chain_nodes.append(data)

            chain_edges: list[dict] = []
            for i in range(len(path_nids) - 1):
                src, tgt = path_nids[i], path_nids[i + 1]
                if G.has_edge(src, tgt):
                    edge = dict(G.edges[src, tgt])
                else:
                    edge = {
                        "distortion_score": None,
                        "tone_shift":       "unknown",
                        "info_lost":        [],
                        "info_added":       [],
                        "is_fact_check":    False,
                    }
                    logger.warning("ArÃªte manquante %s â†’ %s dans le graphe.", src, tgt)
                edge["from_nid"] = src
                edge["to_nid"] = tgt
                chain_edges.append(edge)

            all_paths.append((chain_nodes, chain_edges))
            logger.info(
                "Chemin extrait : %d nÅ“ud(s)  [@%s â†’ â€¦ â†’ @%s]",
                len(chain_nodes),
                chain_nodes[0].get("author_handle", "?"),
                chain_nodes[-1].get("author_handle", "?"),
            )

    # â”€â”€ Fallback : aucun chemin primaireâ†’racine trouvÃ© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if not all_paths:
        root_nid = roots[0]
        data = dict(G.nodes[root_nid])
        data["uri"] = data.get("uri", root_nid)
        all_paths = [([data], [])]
        logger.warning("Aucun chemin primaireâ†’racine â€” graphe rÃ©duit Ã  la racine seule.")

    logger.info("Total : %d chemin(s) extrait(s)", len(all_paths))
    return all_paths


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SÃ©rialisation pour le prompt
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _virality_score(node: dict) -> float:
    """Score de viralitÃ© normalisÃ© [0, 1] via Ã©chelle log10 (plafond 10 000 interactions)."""
    interactions = node.get("likes", 0) + \
        node.get("reposts", 0) + node.get("replies", 0)
    if interactions <= 0:
        return 0.0
    return round(min(1.0, math.log10(interactions + 1) / math.log10(10_001)), 3)


def _serialize_nodes(chain_nodes: list[dict]) -> str:
    """SÃ©rialise les nÅ“uds d'un chemin en texte structurÃ© pour Gemini."""
    lines: list[str] = []

    for i, node in enumerate(chain_nodes):
        if i == 0:
            role = "SOURCE PRIMAIRE"
        elif i == len(chain_nodes) - 1:
            role = "POST ANALYSÃ‰ (racine)"
        else:
            role = f"RELAIS #{i}"

        flags: list[str] = []
        if node.get("is_trusted_source"):
            flags.append("SOURCE DE CONFIANCE")
        if node.get("is_primary"):
            flags.append("AUCUN ANTÃ‰CÃ‰DENT TROUVÃ‰")
        flag_str = "  [" + " | ".join(flags) + "]" if flags else ""

        lines.append(f"â”€â”€ NÅ“ud {i + 1}/{len(chain_nodes)} : {role}{flag_str}")
        lines.append(f"   Auteur      : @{node.get('author_handle', '?')}")
        lines.append(f"   Date        : {node.get('date', '?')}")
        lines.append(
            f"   Engagement  : {node.get('likes', 0)} likes  "
            f"{node.get('reposts', 0)} reposts  "
            f"{node.get('replies', 0)} rÃ©ponses  "
            f"(viralitÃ© normalisÃ©e : {_virality_score(node):.2f}/1.00)"
        )
        fiability = node.get("fiability_score")
        if fiability is not None:
            lines.append(f"   FiabilitÃ©   : {fiability:.2f}/1.00")

        claims = node.get("claims", [])
        if claims:
            lines.append("   Affirmation(s) :")
            for claim in claims:
                lines.append(f"      â€¢ {claim}")

        entities = node.get("entities", [])
        if entities:
            lines.append(f"   EntitÃ©s     : {', '.join(entities)}")

        lines.append(f"   Texte       : \"{node.get('text_preview', '')}\"")
        lines.append("")

    return "\n".join(lines)


def _serialize_edges(chain_nodes: list[dict], chain_edges: list[dict]) -> str:
    """SÃ©rialise les arÃªtes (donnÃ©es Ã©tape 5) en texte structurÃ© pour Gemini."""
    if not chain_edges:
        return "Aucune arÃªte â€” post isolÃ©.\n"

    lines: list[str] = []
    for i, edge in enumerate(chain_edges):
        src_handle = chain_nodes[i].get("author_handle", "?")
        tgt_handle = chain_nodes[i + 1].get("author_handle", "?")
        ds = edge.get("distortion_score")
        ds_str = f"{ds:.2f}" if ds is not None else "N/A (analyse Ã©chouÃ©e)"
        fc = "  âš‘ FACT-CHECK IDENTIFIÃ‰" if edge.get("is_fact_check") else ""

        lines.append(f"  ArÃªte {i + 1} : @{src_handle} â†’ @{tgt_handle}{fc}")
        lines.append(
            f"    distortion_score : {ds_str}  (0=fidÃ¨le, 1=trÃ¨s dÃ©formÃ©)")
        lines.append(
            f"    tone_shift       : {edge.get('tone_shift', 'unknown')}")

        info_lost = edge.get("info_lost",  [])
        info_added = edge.get("info_added", [])
        if info_lost:
            lines.append(
                f"    info_lost        : {json.dumps(info_lost,  ensure_ascii=False)}")
        if info_added:
            lines.append(
                f"    info_added       : {json.dumps(info_added, ensure_ascii=False)}")
        lines.append("")

    return "\n".join(lines)


def _serialize_all_paths(all_paths: list[tuple[list[dict], list[dict]]]) -> str:
    """
    SÃ©rialise l'ensemble des chemins de propagation pour le prompt Gemini.

    Si plusieurs chemins existent (plusieurs sources primaires ou plusieurs
    branches), chacun est clairement dÃ©limitÃ© avec son en-tÃªte.
    """
    if not all_paths:
        return "Aucun chemin trouvÃ© dans le graphe.\n"

    parts: list[str] = []
    for idx, (chain_nodes, chain_edges) in enumerate(all_paths, start=1):
        if len(all_paths) > 1:
            src = chain_nodes[0].get("author_handle", "?")
            tgt = chain_nodes[-1].get("author_handle", "?")
            parts.append(
                f"â•â•â•â•â•â• CHEMIN {idx}/{len(all_paths)} : "
                f"@{src} â†’ @{tgt}  ({len(chain_nodes)} nÅ“ud(s)) â•â•â•â•â•â•\n"
            )
        parts.append(_serialize_nodes(chain_nodes))
        parts.append("DÃ‰FORMATIONS SUR CE CHEMIN :")
        parts.append(_serialize_edges(chain_nodes, chain_edges))

    return "\n".join(parts)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Parsing de la rÃ©ponse LLM
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _parse_distortion(raw: dict, fallback_step: int) -> Distortion:
    try:
        score = max(0.0, min(1.0, float(raw.get("distortion_score", 0.0))))
    except (TypeError, ValueError):
        score = 0.0

    return Distortion(
        step=int(raw.get("step", fallback_step)),
        from_author=str(raw.get("from_author", "inconnu")),
        to_author=str(raw.get("to_author", "inconnu")),
        what_changed=str(raw.get("what_changed", "")),
        info_lost=[str(x) for x in raw.get("info_lost", []) if x],
        info_added=[str(x) for x in raw.get("info_added", []) if x],
        tone_shift=str(raw.get("tone_shift", "neutral")),
        distortion_score=score,
    )


def _parse_response(raw: str) -> Optional[SynthesisResult]:
    """
    Parse la rÃ©ponse JSON de Gemini.
    Retourne None si le parsing est irrÃ©cupÃ©rable.
    """
    fence_match = re.match(
        r'^\s*```(?:json)?\s*\n?(.*?)\n?\s*```\s*$', raw, re.DOTALL)
    if fence_match:
        raw = fence_match.group(1)

    json_match = re.search(r'\{.*\}', raw, re.DOTALL)
    if not json_match:
        logger.warning("Gemini: aucun JSON trouvÃ© dans : %s", raw[:300])
        return None

    try:
        data = json.loads(json_match.group())
    except json.JSONDecodeError as exc:
        logger.warning("Gemini: JSON invalide (%s) : %s", exc, raw[:300])
        return None

    distortions: list[Distortion] = []
    for i, d_raw in enumerate(data.get("distortions", []), start=1):
        try:
            distortions.append(_parse_distortion(d_raw, fallback_step=i))
        except Exception as exc:
            logger.warning("Distortion[%d] invalide (%s) â€” ignorÃ©e.", i, exc)

    try:
        veracity = max(0.0, min(1.0, float(data.get("veracity_score", 0.5))))
    except (TypeError, ValueError):
        veracity = 0.5

    return SynthesisResult(
        original_fact=str(data.get("original_fact", "")),
        final_version=str(data.get("final_version", "")),
        distortions=distortions,
        verdict=str(data.get("verdict", "")),
        veracity_score=veracity,
        narrative=str(data.get("narrative", "")),
        top_amplifiers=[str(h) for h in data.get("top_amplifiers", []) if h],
    )


def _fallback_result(all_paths: list[tuple[list[dict], list[dict]]]) -> SynthesisResult:
    """RÃ©sultat de secours quand Gemini Ã©choue ou renvoie un JSON invalide."""
    if not all_paths:
        return SynthesisResult(
            original_fact="Indisponible",
            final_version="Indisponible",
            verdict="Analyse impossible â€” graphe vide.",
            veracity_score=0.5,
            narrative="L'analyse automatique n'a pas pu Ãªtre effectuÃ©e.",
        )

    chain_nodes, _ = all_paths[0]
    source = chain_nodes[0] if chain_nodes else {}
    root   = chain_nodes[-1] if chain_nodes else {}

    source_claims = source.get("claims", [])
    root_claims   = root.get("claims", [])
    original = source_claims[0] if source_claims else source.get("text_preview", "")
    final    = root_claims[0]   if root_claims   else root.get("text_preview", "")

    return SynthesisResult(
        original_fact=original,
        final_version=final,
        verdict="Analyse automatique indisponible â€” rÃ©sultat de secours.",
        veracity_score=0.5,
        narrative=(
            f"L'analyse narrative n'a pas pu Ãªtre produite automatiquement. "
            f"Le graphe contient {len(all_paths)} chemin(s) de propagation. "
            f"Source primaire : @{source.get('author_handle', '?')}. "
            f"Post analysÃ© : @{root.get('author_handle', '?')}."
        ),
        distortions=[],
        top_amplifiers=[],
    )


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Score de vÃ©racitÃ© structurel â€” donnÃ©es dÃ©jÃ  prÃ©sentes dans le graphe
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _structural_veracity_from_graph(graph) -> float:
    """
    Score de vÃ©racitÃ© structurel calculÃ© Ã  partir des donnÃ©es dÃ©jÃ  stockÃ©es
    dans le ProvenanceGraph â€” sans les recalculer.

    Composantes :
      best_source_quality : fiability_score maximal parmi toutes les sources
                            primaires. Ce score est dÃ©jÃ  calculÃ© par
                            Post_Heuristic_etape7.evaluate_node() lors de
                            la construction du graphe (Ã©tape 6) â€” inutile
                            de le reconstruire ici.
      global_distortion   : distorsion cumulÃ©e sur le chemin le plus distordu,
                            dÃ©jÃ  calculÃ©e par ProvenanceGraph._global_distortion().

    Formule : best_source_quality Ã— (1 âˆ’ global_distortion)

    Returns:
        float in [0.0, 1.0].
    """
    G = graph.G
    if isinstance(G, (nx.MultiDiGraph, nx.MultiGraph)):
        G = nx.DiGraph(G)

    # QualitÃ© des sources primaires â€” lecture directe du fiability_score stockÃ©
    primary_nids = [nid for nid, d in G.nodes(data=True) if d.get("is_primary")]
    if not primary_nids:
        primary_nids = [nid for nid in G.nodes() if not list(G.predecessors(nid))]

    best_quality = max(
        (G.nodes[nid].get("fiability_score", 0.2) for nid in primary_nids),
        default=0.5,
    )

    # Distorsion globale â€” dÃ©jÃ  calculÃ©e par le graphe, pas de doublon
    global_dist = graph._global_distortion()
    if global_dist is None:
        return round(min(1.0, best_quality), 3)

    return round(max(0.0, min(1.0, best_quality * (1.0 - global_dist))), 3)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Analyse structurelle du graphe de provenance complet
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _analyze_graph_structure(graph) -> dict:
    """
    Calcule des mÃ©triques structurelles sur le graphe de provenance complet
    (pas seulement un chemin), afin de contextualiser la propagation.
    """
    G = graph.G
    n = G.number_of_nodes()
    e = G.number_of_edges()

    density = nx.density(G) if n > 1 else 0.0

    in_degrees = [d for _, d in G.in_degree()]
    avg_in_degree = round(sum(in_degrees) / len(in_degrees), 2) if in_degrees else 0.0
    max_in_degree = max(in_degrees) if in_degrees else 0

    try:
        avg_clustering = round(nx.average_clustering(G.to_undirected()), 3)
    except Exception:
        avg_clustering = 0.0

    n_components = nx.number_weakly_connected_components(G) if n > 0 else 1

    return {
        "n_nodes":        n,
        "n_edges":        e,
        "density":        round(density, 4),
        "avg_in_degree":  avg_in_degree,
        "max_in_degree":  max_in_degree,
        "avg_clustering": avg_clustering,
        "n_components":   n_components,
    }


def _serialize_graph_structure(metrics: dict) -> str:
    """Formate les mÃ©triques structurelles en phrase contextuelle pour le prompt LLM."""
    n     = metrics["n_nodes"]
    e     = metrics["n_edges"]
    d     = metrics["density"]
    max_in = metrics["max_in_degree"]
    clust = metrics["avg_clustering"]
    n_comp = metrics["n_components"]

    if d >= 0.5:
        density_interp = "trÃ¨s dense â€” amplification coordonnÃ©e probable, risque Ã©levÃ© de fake news"
    elif d >= 0.15:
        density_interp = "modÃ©rÃ©ment dense â€” propagation ramifiÃ©e avec plusieurs relais croisÃ©s"
    else:
        density_interp = "peu dense â€” propagation majoritairement linÃ©aire"

    if clust >= 0.5:
        clust_interp = "forte tendance aux chambres d'Ã©cho (information recirculant dans un groupe fermÃ©)"
    elif clust >= 0.2:
        clust_interp = "regroupements partiels observÃ©s"
    else:
        clust_interp = "diffusion dispersÃ©e sans clustering significatif"

    if max_in >= 5:
        hub_interp = f"prÃ©sence d'un hub d'amplification majeur (degrÃ© entrant max = {max_in})"
    elif max_in >= 2:
        hub_interp = f"quelques nÅ“uds relais notables (degrÃ© entrant max = {max_in})"
    else:
        hub_interp = "aucun hub dominant identifiÃ©"

    return (
        f"ANALYSE STRUCTURELLE DU GRAPHE DE PROVENANCE ({n} nÅ“ud(s), {e} arÃªte(s)) : "
        f"densitÃ©={d:.3f} ({density_interp}) ; "
        f"clustering moyen={clust:.3f} ({clust_interp}) ; "
        f"{hub_interp} ; "
        f"{n_comp} composante(s) connexe(s). "
        "Prends en compte ces caractÃ©ristiques structurelles dans ton Ã©valuation : "
        "un graphe dense avec fort clustering et hubs actifs est structurellement "
        "propice Ã  l'amplification de dÃ©sinformation, indÃ©pendamment du contenu textuel."
    )


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SÃ©lection et ranking des chemins avant le prompt LLM
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _path_priority_score(chain_nodes: list[dict], chain_edges: list[dict]) -> float:
    """
    Score de prioritÃ© d'un chemin pour sa sÃ©lection dans le prompt Gemini.

    CritÃ¨res (toutes les valeurs viennent du graphe, sans recalcul) :
      - fiability_score de la source (60 %) â€” facteur dominant :
          une source fiable ancre l'analyse sur une vÃ©ritÃ© de rÃ©fÃ©rence.
      - distortion moyenne des arÃªtes (40 %) â€” les chemins trÃ¨s dÃ©formants
          sont prÃ©cieux : ils montrent les Ã©tapes les plus problÃ©matiques.
      - pÃ©nalitÃ© "amplificateur de bruit" : si la source a une faible fiabilitÃ©
          ET une forte viralitÃ©, elle amplifie du bruit sans apporter de vÃ©ritÃ©.
          Ce profil est pÃ©nalisÃ© pour qu'il n'Ã©crase pas les chemins fiables.

    Returns:
        float â€” score composite, plus haut = chemin plus prioritaire.
    """
    source = chain_nodes[0] if chain_nodes else {}
    source_quality  = source.get("fiability_score",  0.2)
    source_virality = source.get("virality_score",   0.0)

    ds_values = [
        e.get("distortion_score")
        for e in chain_edges
        if e.get("distortion_score") is not None
    ]
    avg_distortion = sum(ds_values) / len(ds_values) if ds_values else 0.0

    # PÃ©nalitÃ© amplificateur de bruit : fiabilitÃ© faible + viralitÃ© Ã©levÃ©e
    is_noise_amplifier = (
        source_quality  < _LOW_FIABILITY_THRESH and
        source_virality > _HIGH_VIRALITY_THRESH
    )
    noise_penalty = 0.3 if is_noise_amplifier else 0.0

    return source_quality * 0.6 + avg_distortion * 0.4 - noise_penalty


def _select_paths(
    all_paths: list[tuple[list[dict], list[dict]]],
    max_paths: int = _MAX_PATHS_IN_PROMPT,
) -> list[tuple[list[dict], list[dict]]]:
    """
    SÃ©lectionne les chemins les plus informatifs Ã  envoyer Ã  Gemini.

    Garanties :
      1. Toujours au moins 1 chemin retournÃ©.
      2. Le chemin depuis la source la plus fiable est toujours inclus.
      3. Le chemin le plus distordu (le plus informatif sur les dÃ©formations)
         est toujours inclus, mÃªme s'il vient d'une source peu fiable.
      4. Les chemins restants sont sÃ©lectionnÃ©s par score dÃ©croissant.
      5. Les chemins depuis des sources peu fiables ET trÃ¨s virales
         (amplificateurs de bruit) sont placÃ©s en fin de classement.

    Args:
        all_paths : Tous les chemins extraits par _extract_all_paths().
        max_paths : Nombre maximum de chemins Ã  retenir.

    Returns:
        Sous-liste triÃ©e par pertinence dÃ©croissante.
    """
    if len(all_paths) <= max_paths:
        return all_paths

    # Score chaque chemin
    scored = sorted(
        all_paths,
        key=lambda p: _path_priority_score(*p),
        reverse=True,
    )

    selected: list[tuple[list[dict], list[dict]]] = []

    # Garantie 1 : source la plus fiable
    best_reliable = max(
        all_paths,
        key=lambda p: p[0][0].get("fiability_score", 0.0) if p[0] else 0.0,
    )
    selected.append(best_reliable)

    # Garantie 2 : chemin le plus distordu (informatif sur les dÃ©formations)
    most_distorted = max(
        all_paths,
        key=lambda p: sum(e.get("distortion_score") or 0.0 for e in p[1]),
    )
    if most_distorted not in selected:
        selected.append(most_distorted)

    # ComplÃ©ter jusqu'Ã  max_paths par score dÃ©croissant
    for path in scored:
        if len(selected) >= max_paths:
            break
        if path not in selected:
            selected.append(path)

    dropped = len(all_paths) - len(selected)
    if dropped > 0:
        logger.info(
            "Ã‰tape 9 â€” %d/%d chemin(s) retenus pour le prompt "
            "(%d Ã©cartÃ©(s) : faible fiabilitÃ© / bruit / redondance).",
            len(selected), len(all_paths), dropped,
        )

    return selected


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Fonction principale â€” API publique
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def synthesize(graph, model: Optional[str] = None) -> SynthesisResult:
    """
    Ã‰tape 9 â€” SynthÃ¨se narrative du parcours de l'information.

    Prend un ProvenanceGraph dÃ©jÃ  construit (aprÃ¨s .build()) et appelle
    Gemini pour produire une analyse complÃ¨te couvrant TOUS les chemins
    de propagation identifiÃ©s dans le graphe.

    Args:
        graph : Instance de ProvenanceGraph avec .G (nx.DiGraph) rempli.
        model : Identifiant du modÃ¨le Gemini. Si None, lit SYNTHESIS_MODEL
                depuis l'environnement au moment de l'appel.

    Returns:
        SynthesisResult avec l'analyse complÃ¨te.

    Raises:
        ValueError              : graphe vide ou sans racine identifiable.
        EnvironmentError        : GOOGLE_API_KEY absente.
        genai_errors.ClientError: erreur client non rÃ©cupÃ©rable (auth, quota).
    """
    if model is None:
        model = os.getenv("SYNTHESIS_MODEL", "gemini-2.5-flash")

    logger.info("Ã‰tape 9 â€” synthÃ¨se narrative via %sâ€¦", model)

    all_paths = _extract_all_paths(graph)

    # â”€â”€ Cas trivial : nÅ“ud racine isolÃ© (aucun chemin de propagation) â”€â”€â”€â”€â”€
    if len(all_paths) == 1 and len(all_paths[0][0]) == 1:
        logger.info("Ã‰tape 9 â€” nÅ“ud unique, synthÃ¨se triviale (pas d'appel LLM).")
        node = all_paths[0][0][0]
        claims = node.get("claims", [])
        fact = claims[0] if claims else node.get("text_preview", "")
        is_trusted = node.get("is_trusted_source", False)
        verdict_suffix = " (source de confiance)" if is_trusted else ""

        return SynthesisResult(
            original_fact=fact,
            final_version=fact,
            distortions=[],
            verdict=f"Ce post est la source primaire. Aucune propagation dÃ©tectÃ©e{verdict_suffix}.",
            veracity_score=1.0,
            narrative=(
                f"Le post de @{node.get('author_handle', '?')} est la source originale "
                "de cette information. Aucun relais antÃ©rieur n'a Ã©tÃ© trouvÃ© dans le graphe."
            ),
            top_amplifiers=[],
        )

    # â”€â”€ SÃ©lection des chemins les plus informatifs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # all_paths contient tous les chemins bruts ; selected_paths est le
    # sous-ensemble envoyÃ© Ã  Gemini (chemins les plus fiables/distordus,
    # amplificateurs de bruit Ã©cartÃ©s si des alternatives existent).
    selected_paths = _select_paths(all_paths)

    # â”€â”€ Construction du prompt â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    all_paths_text  = _serialize_all_paths(selected_paths)
    graph_structure = _serialize_graph_structure(_analyze_graph_structure(graph))
    prompt = _USER_TEMPLATE.format(
        all_paths_text=all_paths_text,
        graph_structure=graph_structure,
    )
    client = _get_client()

    logger.info(
        "Ã‰tape 9 â€” appel Gemini (%d/%d chemin(s) sÃ©lectionnÃ©(s), prompt ~%d chars)â€¦",
        len(selected_paths), len(all_paths), len(prompt),
    )

    for attempt in range(_MAX_RETRIES):
        try:
            response = client.models.generate_content(
                model=model,
                contents=prompt,
                config=genai_types.GenerateContentConfig(
                    system_instruction=_SYSTEM_PROMPT,
                    temperature=0.0,
                    max_output_tokens=8192,
                ),
            )
            raw = response.text.strip()
            result = _parse_response(raw)

            if result is None:
                logger.warning("Ã‰tape 9 â€” parsing Ã©chouÃ©, retour du rÃ©sultat de secours.")
                return _fallback_result(selected_paths)

            # â”€â”€ Ancrage objectif du veracity_score â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # Score structurel : utilise les donnÃ©es dÃ©jÃ  dans le graphe
            # (fiability_score des nÅ“uds + _global_distortion du graphe).
            # Score LLM        : Ã©valuation sÃ©mantique de Gemini.
            # Blend 50/50      : combine rigueur objective et nuance sÃ©mantique.
            structural = _structural_veracity_from_graph(graph)
            blended = round(0.5 * structural + 0.5 * result.veracity_score, 3)
            logger.info(
                "Ã‰tape 9 â€” veracity : structurel=%.3f  LLM=%.3f  pondÃ©rÃ©=%.3f",
                structural, result.veracity_score, blended,
            )
            result.veracity_score = blended

            logger.info(
                "Ã‰tape 9 â€” OK : veracity=%.2f, %d dÃ©formation(s), %d amplificateur(s)",
                result.veracity_score,
                len(result.distortions),
                len(result.top_amplifiers),
            )
            return result

        except genai_errors.ClientError:
            raise

        except genai_errors.ServerError as exc:
            if attempt == _MAX_RETRIES - 1:
                logger.warning(
                    "Ã‰tape 9 â€” %d tentatives Ã©chouÃ©es (%s), fallback.", _MAX_RETRIES, exc
                )
                return _fallback_result(selected_paths)
            delay = _RETRY_BASE_DELAY * (2 ** attempt)
            logger.warning(
                "Ã‰tape 9 â€” erreur transitoire (%s) â€” retry %d/%d dans %.1f s.",
                exc, attempt + 1, _MAX_RETRIES, delay,
            )
            time.sleep(delay)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Smoke test local
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if __name__ == "__main__":
    import sys

    logging.basicConfig(level=logging.INFO, format="%(levelname)s %(message)s")

    if not os.environ.get("GOOGLE_API_KEY"):
        print("GOOGLE_API_KEY absente â€” test Gemini rÃ©el ignorÃ©.")
        sys.exit(0)

    import glob as _glob
    json_files = sorted(_glob.glob("provenance_*.json"))
    if not json_files:
        print("Aucun fichier provenance_*.json trouvÃ©.")
        sys.exit(1)

    target = json_files[-1]
    print(f"Chargement : {target}")
    with open(target) as f:
        data = json.load(f)

    G = nx.node_link_graph(data, directed=True, edges="links")

    graph = ProvenanceGraph()
    graph.G = G

    print("\n[Gemini] appel rÃ©el en coursâ€¦")
    _result_real = synthesize(graph)
    _result_real.print_summary()
    print(json.dumps(_result_real.to_dict(), ensure_ascii=False, indent=2))
