"""
validate_candidates_etape4.py â€” Ã‰tape 4 de la pipeline
=======================================================
Validation sÃ©mantique des posts candidats via Gemini 2.5 Flash.

Pour chaque candidat, Gemini Ã©value si le post candidat et le post de rÃ©fÃ©rence
rapportent le MÃŠME FAIT ou la MÃŠME AFFIRMATION principale.

  SCORE FINAL = llm_score retournÃ© par Gemini (0.0 â†’ 1.0)
  Retour      : list[ValidationResult] filtrÃ©s Ã  â‰¥ FINAL_THRESHOLD,
                triÃ©s par score dÃ©croissant.

DÃ©pendances :
    pip install google-genai

Variables d'environnement :
    GOOGLE_API_KEY   â€” clÃ© API Gemini (obligatoire)
    VALIDATE_MODEL   â€” modÃ¨le Gemini (dÃ©faut : gemini-2.5-flash)
    VALIDATE_WORKERS â€” nombre de threads parallÃ¨les (dÃ©faut : 4)
"""

from __future__ import annotations
from dataclasses import dataclass
from typing import Optional

import json
import logging
import os
import re
from concurrent.futures import ThreadPoolExecutor, as_completed

from google import genai
from google.genai import types as genai_types

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Configuration
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Seuil de validation finale. Conforme Ã  MIN_SIMILARITY de provenance_graph.py.
FINAL_THRESHOLD = 0.65

# ParallÃ©lisme des appels LLM (ThreadPoolExecutor)
DEFAULT_WORKERS = int(os.getenv("VALIDATE_WORKERS", "4"))

# ModÃ¨le Gemini
GEMINI_MODEL = os.getenv("VALIDATE_MODEL", "gemini-2.5-flash")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Logging
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

logger = logging.getLogger(__name__)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# RÃ©sultat de validation (public â€” utilisÃ© par Provenance_graph_etape6.py)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


@dataclass
class ValidationResult:
    """RÃ©sultat de validation pour un post candidat."""
    post: object              # BlueskyPost
    similarity_score: float   # Score final 0.0â€“1.0
    same_topic: bool          # Le LLM considÃ¨re-t-il que c'est le mÃªme fait ?
    reasoning: str            # Justification textuelle du LLM


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Client Gemini (singleton)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

_gemini_client: Optional[genai.Client] = None


def _get_gemini_client() -> genai.Client:
    """Charge le client Gemini de faÃ§on paresseuse (lazy singleton)."""
    global _gemini_client
    if _gemini_client is None:
        api_key = os.getenv("GOOGLE_API_KEY")
        if not api_key:
            raise EnvironmentError(
                "GOOGLE_API_KEY non dÃ©finie. "
                "La passe LLM (Ã©tape 4) nÃ©cessite une clÃ© Gemini valide."
            )
        _gemini_client = genai.Client(api_key=api_key)
    return _gemini_client


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Prompts
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

_LLM_SYSTEM_PROMPT = """Tu es un expert en analyse de dÃ©sinformation et de propagation de l'information sur les rÃ©seaux sociaux.

Ta tÃ¢che est de comparer deux posts Bluesky et de dÃ©terminer s'ils parlent du MÃŠME FAIT ou de la MÃŠME AFFIRMATION principale.

RÃ¨gles d'Ã©valuation :
- Concentre-toi sur l'AFFIRMATION FACTUELLE centrale, pas sur la forme ou le style.
- Deux posts trÃ¨s diffÃ©rents dans la forme mais rapportant le mÃªme fait â†’ score Ã©levÃ©.
- Une reformulation biaisÃ©e du mÃªme fait doit recevoir un score Ã©levÃ© (c'est prÃ©cisÃ©ment ce qu'on cherche Ã  dÃ©tecter).
- Des posts sur des sujets superficiellement similaires mais avec des faits diffÃ©rents â†’ score bas.

RÃ©ponds UNIQUEMENT en JSON valide, sans texte autour :
{
  "same_topic": true,
  "similarity_score": 0.0,
  "reasoning": "explication concise en franÃ§ais (max 80 mots)",
  "shared_entities": ["entitÃ©1", "entitÃ©2"],
  "shared_claims": ["affirmation1"]
}"""

_USER_TEMPLATE = """POST DE RÃ‰FÃ‰RENCE (plus rÃ©cent) :
\"\"\"{ref_text}\"\"\"

POST CANDIDAT (antÃ©rieur) :
\"\"\"{candidate_text}\"\"\"

Ces deux posts rapportent-ils le mÃªme fait ou la mÃªme affirmation principale ?"""


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Parsing de la rÃ©ponse LLM
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _parse_llm_response(raw: str) -> tuple[float, bool, str]:
    json_match = re.search(r'\{.*\}', raw, re.DOTALL)
    if not json_match:
        logger.warning(
            "LLM: aucun JSON trouvÃ© dans la rÃ©ponse : %s", raw)
        return -1.0, False, "parse_error"

    try:
        data = json.loads(json_match.group())
    except json.JSONDecodeError as exc:
        logger.warning("LLM: JSON invalide (%s) : %s", exc, raw[:200])
        return -1.0, False, "json_decode_error"

    # â† extract all fields before returning
    score_raw = data.get("similarity_score", 0.0)
    try:
        score = max(0.0, min(1.0, float(score_raw)))
    except (TypeError, ValueError):
        logger.warning("LLM: similarity_score invalide : %s", score_raw)
        return -1.0, False, "invalid_score"

    # â† key aligns with prompt JSON
    same_claim = bool(data.get("same_topic", False))
    reasoning = str(data.get("reasoning", ""))
    return score, same_claim, reasoning


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Appel LLM â€” Gemini
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _call_gemini_single(
    client: genai.Client,
    ref_text: str,
    candidate_text: str,
    candidate_idx: int,
) -> tuple[int, float, bool, str]:
    """
    Appel Gemini pour une seule paire (ref, candidat).
    Retourne (candidate_idx, similarity_score, same_claim, reasoning).
    similarity_score == -1.0 en cas d'Ã©chec.
    """
    prompt = _USER_TEMPLATE.format(
        ref_text=ref_text.strip(),
        candidate_text=candidate_text.strip(),  # â† was candidate.text.strip()
    )
    try:
        response = client.models.generate_content(
            model=GEMINI_MODEL,
            contents=prompt,
            config=genai_types.GenerateContentConfig(
                system_instruction=_LLM_SYSTEM_PROMPT,
                temperature=0.0,
                max_output_tokens=2000,
            ),
        )
        raw = response.text.strip() if response.text else ""
        score, same_claim, reasoning = _parse_llm_response(raw)
        logger.debug(
            "LLM[%d]: score=%.2f  same_claim=%s  reasoning=%s",
            candidate_idx, score, same_claim, reasoning[:60]
        )
        return candidate_idx, score, same_claim, reasoning

    except Exception as exc:
        logger.warning("LLM[%d]: appel Ã©chouÃ© (%s)", candidate_idx, exc)
        print(f"   âš ï¸  Gemini[{candidate_idx}] erreur : {exc}")
        return candidate_idx, -1.0, False, f"api_error: {exc}"


def _pass_gemini_parallel(
    ref_text: str,
    candidates: list[tuple[int, str]],   # [(original_idx, text), ...]
    workers: int = DEFAULT_WORKERS,
) -> dict[int, tuple[float, bool, str]]:
    """
    Lance les appels Gemini en parallÃ¨le pour tous les candidats.
    Retourne un dict {original_idx: (score, same_claim, reasoning)}.
    """
    client = _get_gemini_client()
    results: dict[int, tuple[float, bool, str]] = {}

    with ThreadPoolExecutor(max_workers=workers) as executor:
        futures = {
            executor.submit(
                _call_gemini_single, client, ref_text, text, orig_idx
            ): orig_idx
            for orig_idx, text in candidates
        }

        for future in as_completed(futures):
            orig_idx = futures[future]
            try:
                _, score, same_claim, reasoning = future.result()
            except Exception as exc:
                logger.error("Future[%d] inattendue : %s", orig_idx, exc)
                score, same_claim, reasoning = -1.0, False, "unexpected_error"

            results[orig_idx] = (score, same_claim, reasoning)
            logger.info(
                "  Candidat[%d] â†’ score=%.2f  same_claim=%s  (%s)",
                orig_idx, score, same_claim, reasoning[:50]
            )

    return results


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Fonction principale â€” API publique
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def validate_candidates(
    ref_post,
    candidates: list,
    final_threshold: float = FINAL_THRESHOLD,
    workers: int = DEFAULT_WORKERS,
) -> list[ValidationResult]:
    """
    Ã‰tape 4 â€” Validation sÃ©mantique des posts candidats via Gemini 2.5 Flash.

    Args:
        ref_post   : BlueskyPost de rÃ©fÃ©rence (le post dont on cherche les antÃ©cÃ©dents).
                     Doit exposer un attribut `.text` (str).
        candidates : Liste de BlueskyPost candidats (issus de l'Ã©tape 3).
                     Chaque objet doit exposer `.text`.
        final_threshold :
                     Seuil de validation (score Gemini).
                     DÃ©faut : 0.65, conforme Ã  MIN_SIMILARITY du graphe.
        workers    : Nombre de threads parallÃ¨les pour les appels LLM.

    Returns:
        list[ValidationResult] pour les candidats validÃ©s,
        triÃ©e par similarity_score dÃ©croissant.
        Seuls les candidats avec similarity_score >= final_threshold sont inclus.

    Raises:
        EnvironmentError : si GOOGLE_API_KEY n'est pas dÃ©finie.
    """
    if not candidates:
        print("[Ã‰tape 4] Aucun candidat â†’ retour vide.")
        return []

    ref_text = ref_post.text
    candidate_texts = [c.text for c in candidates]  # â† was missing

    print(
        f"[Ã‰tape 4] ðŸ§  Validation Gemini sur {len(candidates)} candidat(s) "
        f"(workers={workers}, model={GEMINI_MODEL})â€¦"
    )

    indexed_candidates = list(enumerate(candidate_texts))

    llm_results = _pass_gemini_parallel(
        ref_text, indexed_candidates, workers=workers)

    validated: list[ValidationResult] = []

    for idx, (post, text) in enumerate(zip(candidates, candidate_texts)):
        score, same_claim, reasoning = llm_results.get(
            idx, (-1.0, False, "not_called"))

        final_score = score if score >= 0.0 else 0.0

        status = "âœ“ VALIDÃ‰" if final_score >= final_threshold else "âœ— rejetÃ©"
        print(
            f"   [{idx}] score={final_score:.2f}  same_claim={same_claim}"
            f"  @{getattr(post, 'author_handle', '?')} â†’ {status}"
        )

        if final_score >= final_threshold:
            validated.append(ValidationResult(
                post=post,
                similarity_score=round(final_score, 4),
                same_topic=same_claim,
                reasoning=reasoning,
            ))

    validated.sort(key=lambda r: r.similarity_score, reverse=True)

    print(
        f"[Ã‰tape 4] âœ… {len(validated)}/{len(candidates)} candidat(s) validÃ©(s) "
        f"(seuil={final_threshold})"
    )
    return validated

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Smoke test (sans dÃ©pendance externe sauf GOOGLE_API_KEY)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


if __name__ == "__main__":
    """
    Smoke test â€” valide un appel Gemini rÃ©el.
    Lance : python validate_candidates_etape4.py
    """
    logging.basicConfig(level=logging.INFO, format="%(levelname)s %(message)s")

    class _FakePost:
        def __init__(self, text: str, handle: str = "user"):
            self.text = text
            self.author_handle = handle
            self.uri = f"at://fake/{handle}"
            self.date = "2025-01-01T00:00:00Z"

    ref = _FakePost(
        "Des manifestants arborant des croix gammÃ©es ont Ã©tÃ© photographiÃ©s lors "
        "du rassemblement de Lyon selon le journaliste de BFM prÃ©sent sur place.",
        handle="ref_post",
    )
    candidates = [
        _FakePost(
            # TrÃ¨s similaire â€” doit Ãªtre validÃ©
            "Le correspondant de BFM TV Ã  Lyon confirme avoir vu des symboles nazis "
            "parmi les participants au rassemblement.",
            handle="similar_post",
        ),
        _FakePost(
            # Sujet diffÃ©rent â€” doit Ãªtre rejetÃ©
            "Recette de quiche lorraine maison : 200g de lardons, 3 Å“ufs, 20cl de crÃ¨me.",
            handle="unrelated_post"
        ),
        _FakePost(
            # ModÃ©rÃ©ment similaire
            "Les organisateurs du rassemblement lyonnais dÃ©mentent toute prÃ©sence "
            "d'Ã©lÃ©ments d'extrÃªme droite lors de l'Ã©vÃ©nement.",
            handle="moderate_post"
        ),
    ]

    print("\n" + "="*60)
    print(f"SMOKE TEST â€” validate_candidates_etape4.py")
    print(f"ModÃ¨le : {GEMINI_MODEL}")
    print("="*60 + "\n")

    results = validate_candidates(ref, candidates)
    print(f"\nâ†’ {len(results)} candidat(s) validÃ©(s) :\n")
    for r in results:
        print(f"  [{r.similarity_score:.4f}]  @{r.post.author_handle}")
        print(f"    same_topic={r.same_topic}  reasoning: {r.reasoning[:80]}")
